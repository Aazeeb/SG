27-Apr

Simple Linear Regression - There is one input column which is used to predict the output
Multi Linear Regression - Multiple independent variables(multiple input columns)

Criteria for applying linear regression:
The input and output should have almost a linear relationship

Metrics:
Regression
-Mean Absolute Error
-Mean Squared Error
-Root Mean Squared Error
-R2 Score
-Adjusted R2 Score

We use the errors to Optimize the model 

A few models use Differentiation in optimation techniques
hence MAE is not very effective
so, the need for newer metric, hence MSE

RMSE for better readability
---------------------------------------------

Bias
Variance
Overfitting
Underfitting
Regularization
-L1 Regularization
-L2 Regularization
Bagging
Boosting
---------------------------
overfitting - the model is accurately following the training data, but not having desired results on testing data.
underfitting- the model does not learn the training properly and hence will nonot be able to predict the testing data properly as well.

Bias- Inability of the model to learn from training data
variance-Difference of errors on various types of data

An ideal ML model should have:
Low Bias
Low Variance

Bias-Variance tradeoff
-Regularization
-Bagging
-Boosting
----------------------

L2 Regularization
Ridge Regression
E= sum(y-y_pred)^2 + lambda.m2  , where m=slope, lamba=hyperparameter


range of lambda is 0 to infinity

GridSearchCV - tune the hyperparamter

m =  ("∑(x−" 𝑥′")(y−" 𝑦′") " )/(("∑(x−" 𝑥′")" 2) +lambda)


L1 Regularization
Lasso Regression
lambda.|m|

-------------------------------------

Feature Engineering
-Removing unneccesary features
-Select best features
-Feature Extraction: We create new features from the existing features

Imputing missing values:
a few rules we will assume
if the percentage of missing values is between 0 and 20, fill it with mean, median or mode
if the percentage of missing values is above 60%, remove the column because there is lots of missing data
if the percentage of missing values is between 20 and 60, look for advance imputing techniques like: knn imputer,etc.
-----------------------
split the data into training and testing
some part of data into training
training data will be fed to the machine
after machine is trained, we will validate it using the testing data


------------------------
02 May

Encoding - to convert values from object datatype to numerical datatype
Input
Output

For output value being non-numeric, we use Label Encoder
For input value being non-numeric, there could be two types of data
1. Ordinal
2. Nominal
For ordinal values, we use ordinal encoding, which will give each category the numerical predence based on the value it deserves
For nominal values, we use one hot encoding
----------------------------------

Linear Regression
Logistic Regression
Clustering : Kmeans
Decision Trees
SVM - Support Vector machines
Association
--------------------------------------
KMeans
K- no of clusters

k is determined by a few factors listed below
- k should never be 1
- k ideally should not exceed the square root of total number of points
- k value could be found by using a statistical technique called as elbow method
-Elbow method is a grpahical representation, where in we plot
SSE on y-axis and no. of clusters on x-axis.
Wherever the graph makes an elbow like point, that is taken as the value of k
----------------
04-May

Clustering Metric
Silhoeutte Score
------------------
Logistic Regression

Requirement:
We should be able to linear separate the data by drawing a straight line(2D), plane(3D), hyperplane(higher dimension)
if the above condition is satisfied, we can use logistic regression to classify the data
We find the equation of line
AX+By+C=0		(2D)
Ax+By+Cz+D=0	(3D)

we should be able to find A, B and C.

taking the example
cgpa	iq	placement
7.5	61	1
8.9	109	1
7	81	0

x=cgpa, y=iq
s1=A(7.5)+B(61)=?
if s1>=0, then student will be placed
if s1<0, then student will not be placed

but, there the values could range from -infinity to infinity

this value will be passed tthrough a sigmoid function
Sigmoid function will converts the value in the range to 0 to 1.

s(x)=1/(1+(e^-x))

100

0 and 1
s(x)>0.5, then prediction is +ve
if s(x)<0.5, then prediction is -ve
-------------------
Standardization -1 to 1
Normalization 0 to 1
-----------------------------
11-May-2022

Classification metrics:
Accuracy score
Confusion Matrix
Classification report
Precision
Recall
F1 Score
Area Under the CUrve(AUC)
--------------------------
Regression - Linear, Decision Tree
Classification - Logistic Regression, Decision Trees
Clustering - KMeans
------------------

Precision can be seen as a measure of quality, and recall as a measure of quantity

-----------------------------------------

Decision Tree Algorithm


CART Algorithm(Classification and Regression Trees)
Decision tree is simply a collection of multiple conditional statements(if else loops)

--
-Begin with the training dataset, which should have some features and the output should have classification or regression values
-Determine the best feature in the data to split the data on. we will look at how to determine the best feature later
-Split the data into subsets that contain the correct values for this best feature. This splitting basically defines each node on the tree
-Recursively generate new decision nodes until we get to the leaf node

---Conclusions
-Programatically speaking, decision trees are nothing but a giant structure of nested if-else condition
-Mathematically speaking, decision trees uses lines/planes/hyperplanes which run parallel to any one of the axis to cut the co-ordinate system into rectangles/cuboids/hypercuboids

--some unanswered questions
-How to find which column should be selected as root node?
-How to select subsequent decision nodes?
-How to decide splitting criteria in case of numerical columns?

---Advantages of CART
-Intuitive and easy to understand
-Minimal data preparation is required

---Disadvantages of CART
-Overfitting
-Prone to errors for imbalanced datasets
-------
Entropy and Information Gain

Entropy: Entropy is nothing but a measure of disorder. It is also called as measure of purity/impurity


Ice, water, vapor

---IG
Information Gain, is a metric used to train decision trees.
Specifically, this metric measures the quality of the split

IG is inversely proportional to Entropy
whenever we are contructing a decision tree, we select the feature having highest IG as the root node

--------------
Hyperparameter tuning


IG= E(Parent)-(Weighted Average)*E(Children)

---Gini Impurity
We tend to use Gini more, because it is easier to calculate

E=-Py(log(Py)-Pn(log(Pn)
G=1-(Py^2+Pn^2)


----------------------------
30 may

Stop words

Recommender System

-----------------
Association ruling
baby products, milk and diapers
A and B are bought together more frequently, then several steps can be taken to increase the profit
(1) A and B can be placed together
(2) Targeted ads on product B to people who buy product A
(3) Collective discounts
(4) Both A and B can be packaged together

Association rule mining
Apriori Algorithm

3 major components
(1) Support
(2) Confidence
(3) Lift

1000 customer transactions
out of 1000, 100 contain ketchup and 150 of them contains burgers
out of 150, 50 contain ketchup as well

Support: popularity of an item

Support(B) = Transcations containing B/total transactions

support(ketchup)=100/1000= 0.10=10%

Confidence
It is likelihood that item B is also bought when item A is bought
condifende(A->B)= transactions involving A and B both/transaction of A

Likelihood of buying ketcup when burger is bought
confidence(Burger->Ketchup)=50/150=33%

Lift
Lift(A->B) refers to increase in ratio of sale of B when A is sold
lift(A->B)=confidence(A->B)/support(B)
=(33%)/10%
=3.33
Likelihood of buying a burger and ketchup together is 3.33 times more than the likelihood of buying the ketchup alone.

Apriori Algorithm